{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f444b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "spark=SparkSession.builder.appName(\"Read data\").getOrCreate()\n",
    "\n",
    "bucket_path = 'gs://iskldl01-projectby-local-bucket'\n",
    "source = 'source'\n",
    "target= 'raw'\n",
    "\n",
    "#Result structure:\n",
    "result_schema = StructType(\n",
    "    [\n",
    "        StructField(\"Table\", StringType(), True),\n",
    "        StructField(\"DQ check\", StringType(), True),\n",
    "        StructField(\"Column\", StringType(), True),\n",
    "        StructField(\"Status\", StringType(), True),\n",
    "        StructField(\"Bad data\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create an empty DataFrame with the specified schema\n",
    "result_df = spark.createDataFrame([], result_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b2c6416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create temporary tables from source data\n",
    "src_airports=spark.read.option(\"header\", \"true\").csv(f\"{bucket_path}/{source}/airports.csv\")\n",
    "src_airports.createOrReplaceTempView(\"src_airports\")\n",
    "\n",
    "src_carriers=spark.read.option(\"header\", \"true\").csv(f\"{bucket_path}/{source}/carriers.csv\")\n",
    "src_carriers.createOrReplaceTempView(\"src_carriers\")\n",
    "\n",
    "src_flights=spark.read.option(\"header\", \"true\").csv(f\"{bucket_path}/{source}/flights.csv\")\n",
    "src_flights.createOrReplaceTempView(\"src_flights\")\n",
    "\n",
    "#Create temporary tables from raw data\n",
    "raw_airports=spark.read.option(\"header\", \"true\").parquet(f\"{bucket_path}/{target}/airports\")\n",
    "raw_airports.createOrReplaceTempView(\"raw_airports\")\n",
    "\n",
    "raw_carriers=spark.read.option(\"header\", \"true\").parquet(f\"{bucket_path}/{target}/carriers\")\n",
    "raw_carriers.createOrReplaceTempView(\"raw_carriers\")\n",
    "\n",
    "raw_flights=spark.read.option(\"header\", \"true\").parquet(f\"{bucket_path}/{target}/flights\")\n",
    "raw_flights.createOrReplaceTempView(\"raw_flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c5691054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check completeness regardless of the object\n",
    "def ck_completeness(object_name):\n",
    "    global result_df\n",
    "    #Define column set\n",
    "    #Ideally, this would be pulled directly from a DM, source names mapped to target, etc.\n",
    "    column_list = []\n",
    "    if object_name == 'carriers':\n",
    "        column_list = ['Code', 'Description']\n",
    "    elif object_name == 'flights':\n",
    "        column_list = ['Year','Month','DayofMonth','DayofWeek','DepTime','CRSDepTime','ArrTime','CRSArrTime','UniqueCarrier','FlightNum','TailNum','ActualElapsedTime','CRSElapsedTime','AirTime','ArrDelay','DepDelay','Origin','Dest','Distance','TaxiIn','TaxiOut','Cancelled','CancellationCode','Diverted','CarrierDelay','WeatherDelay','NASDelay','SecurityDelay','LateAircraftDelay']\n",
    "    elif object_name == 'airports':\n",
    "        column_list = ['iata','airport','city','state','country','lat','long']\n",
    "    else:\n",
    "        column_list = ['1']\n",
    "\n",
    "    column_string = ','.join([f\"NULLIF(TRIM({column}),'') as {column}\" for column in column_list])\n",
    "\n",
    "    #Select all from tables with the same order of columns\n",
    "    src_rbr = spark.sql(f'select {column_string} from src_{object_name}')\n",
    "    if object_name == 'airports':\n",
    "        column_string_ra = column_string.replace('long','longt')\n",
    "        raw_rbr = spark.sql(f'select {column_string_ra} from raw_{object_name}')\n",
    "    else:\n",
    "        raw_rbr = spark.sql(f'select {column_string} from raw_{object_name}')\n",
    "\n",
    "    #Row-by-row check\n",
    "    src_raw = src_rbr.exceptAll(raw_rbr)\n",
    "    raw_src = raw_rbr.exceptAll(src_rbr)\n",
    "\n",
    "    #Assessment of test execution\n",
    "    if src_raw.count() == 0 and raw_src.count()==0 and src_rbr.count()==raw_rbr.count():\n",
    "        Status = 'Passed'\n",
    "        bad_data_list=''\n",
    "    else:\n",
    "        Status = 'Failed'\n",
    "        bad_data_list = [[f\"{src_raw.columns[i]}={row[i]}\" for i in range(len(src_raw.columns))] for row in src_raw.collect()]\n",
    "        # Add raw_src for reference\n",
    "        bad_data_list += [[f\"{raw_src.columns[i]}={row[i]}\" for i in range(len(raw_src.columns))] for row in raw_src.collect()]\n",
    "\n",
    "\n",
    "    # Create a DataFrame to store the result\n",
    "    result_complete = [\n",
    "        (\n",
    "            object_name,\n",
    "            'Completeness',\n",
    "            'All',\n",
    "            Status,\n",
    "            bad_data_list,\n",
    "        )\n",
    "    ]\n",
    "    res_to_add = spark.createDataFrame(result_complete, result_schema)\n",
    "\n",
    "    # Append the data DataFrame to the empty_result_df\n",
    "    result_df = result_df.union(res_to_add) \n",
    "    result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "032441e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+------+--------+\n",
      "|   Table|    DQ check|Column|Status|Bad data|\n",
      "+--------+------------+------+------+--------+\n",
      "|carriers|Completeness|   All|Passed|        |\n",
      "+--------+------------+------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#will pass if function only has PK columns - decided to include Row-by-row\n",
    "ck_completeness('carriers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4081aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ck_consistency(object_name, field_name):\n",
    "    global result_df\n",
    "    try:\n",
    "        if object_name == 'airports' and field_name == 'state':\n",
    "            raw_st_cry = spark.sql(f\"select iata, NULLIF(state,'') as state, country from raw_{object_name}\")\n",
    "            #List of US states\n",
    "            us_states = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "            bad_data_message = ''\n",
    "            for row in raw_st_cry.collect():\n",
    "                country = row['country']\n",
    "                state = row['state']\n",
    "                iata = row['iata']\n",
    "                if country=='USA':\n",
    "                    if state=='NA' or state=='':\n",
    "                        bad_data_message += f'Should be a US state but is NULL. Iata: {iata},\\n'\n",
    "                    elif state not in us_states:\n",
    "                        bad_data_message += f'State not in the US. Iata: {iata},\\n'\n",
    "                elif country != 'USA':\n",
    "                    if state!='NA':\n",
    "                        bad_data_message += f'Invalid state value. Iata: {iata},\\n'\n",
    "\n",
    "            if bad_data_message != '':\n",
    "                Status='Failed'\n",
    "            else:\n",
    "                Status='Passed'\n",
    "\n",
    "\n",
    "        elif object_name == 'flights' and field_name == 'CancellationCode':\n",
    "            raw_data = spark.sql(f\"select Year, Month, DayofMonth, FlightNum, Cancelled, CancellationCode from raw_{object_name}\")\n",
    "            accepted_vals = ['A','B','C']\n",
    "            bad_data_message = ''\n",
    "            for row in raw_data.collect():\n",
    "                cancelled = row[\"Cancelled\"]\n",
    "                canc_code = row[\"CancellationCode\"]\n",
    "                if cancelled == '0':\n",
    "                    if canc_code !='':\n",
    "                        bad_data_message += f'Invalid value. Should be Null. Row: {row}'\n",
    "                if cancelled == '1':\n",
    "                    if canc_code not in accepted_vals:\n",
    "                        bad_data_message += f'Invalid value. Should be A, B, or C. Row: {row}'\n",
    "\n",
    "            if bad_data_message != '':\n",
    "                Status='Failed'\n",
    "            else:\n",
    "                Status='Passed'\n",
    "\n",
    "\n",
    "        elif object_name == 'flights' and field_name == 'CRSElapsedTime':\n",
    "            raw_data = spark.sql(f\"select Year, Month, DayofMonth, FlightNum, CRSArrTime, CRSDepTime, CRSElapsedTime from raw_{object_name}\")\n",
    "            bad_data_message = ''\n",
    "            for row in raw_data.collect():\n",
    "                try:\n",
    "                    arr_time = int(row[\"CRSArrTime\"])\n",
    "                    dep_time = int(row[\"CRSDepTime\"])\n",
    "                    act_diff = int(row[\"CRSElapsedTime\"])\n",
    "                except Exception as e:\n",
    "                    bad_data_message += f\"An error occurred: {e}. Row: {row}\"\n",
    "\n",
    "                if arr_time > dep_time:\n",
    "                    arr_min = arr_time//100*60 + arr_time%100\n",
    "                    dep_min = dep_time//100*60 + dep_time%100\n",
    "                    exp_diff = arr_min - dep_min\n",
    "                elif arr_time < dep_time:\n",
    "                    arr_min = (24-arr_time//100)*60\n",
    "                    dep_min = dep_time//100*60 + dep_time%100\n",
    "                    exp_diff = arr_min + dep_min\n",
    "\n",
    "                test_diff = exp_diff - act_diff\n",
    "\n",
    "                if test_diff != 0:\n",
    "                    bad_data_message += f'Inconsitent value of CRSElapsedTime. Actual: {act_diff}, Expected: {exp_diff}. Row: {row}.'\n",
    "\n",
    "            if bad_data_message != '':\n",
    "                Status='Failed'\n",
    "            else:\n",
    "                Status='Passed'\n",
    "\n",
    "        else:\n",
    "            print('Not yet included in the test')\n",
    "\n",
    "        # Create a DataFrame to store the result\n",
    "        result_consist = [\n",
    "            (\n",
    "                object_name,\n",
    "                'Consistency',\n",
    "                field_name,\n",
    "                Status,\n",
    "                bad_data_message,\n",
    "            )\n",
    "        ]\n",
    "        res_to_add = spark.createDataFrame(result_consist, result_schema)\n",
    "\n",
    "        # Append the data DataFrame to the empty_result_df\n",
    "        result_df = result_df.union(res_to_add) \n",
    "        result_df.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Not yet included in the test. {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8a167546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+----------------+------+--------------------+\n",
      "|   Table|    DQ check|          Column|Status|            Bad data|\n",
      "+--------+------------+----------------+------+--------------------+\n",
      "|carriers|Completeness|             All|Passed|                    |\n",
      "| flights| Consistency|CancellationCode|Failed|Invalid value. Sh...|\n",
      "+--------+------------+----------------+------+--------------------+\n",
      "\n",
      "+--------+------------+----------------+------+--------------------+\n",
      "|   Table|    DQ check|          Column|Status|            Bad data|\n",
      "+--------+------------+----------------+------+--------------------+\n",
      "|carriers|Completeness|             All|Passed|                    |\n",
      "| flights| Consistency|CancellationCode|Failed|Invalid value. Sh...|\n",
      "| flights| Consistency|  CRSElapsedTime|Failed|Inconsitent value...|\n",
      "+--------+------------+----------------+------+--------------------+\n",
      "\n",
      "+--------+------------+----------------+------+--------------------+\n",
      "|   Table|    DQ check|          Column|Status|            Bad data|\n",
      "+--------+------------+----------------+------+--------------------+\n",
      "|carriers|Completeness|             All|Passed|                    |\n",
      "| flights| Consistency|CancellationCode|Failed|Invalid value. Sh...|\n",
      "| flights| Consistency|  CRSElapsedTime|Failed|Inconsitent value...|\n",
      "|airports| Consistency|           state|Failed|Should be a US st...|\n",
      "+--------+------------+----------------+------+--------------------+\n",
      "\n",
      "Not yet included in the test\n",
      "Not yet included in the test. local variable 'Status' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "ck_consistency('flights', 'CancellationCode')\n",
    "ck_consistency('flights', 'CRSElapsedTime')\n",
    "ck_consistency('airports', 'state')\n",
    "ck_consistency('airports', 'check_other')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local PySpark (Python-3.7.9 / Spark-3.0.1 )",
   "language": "python",
   "name": "py3spark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
